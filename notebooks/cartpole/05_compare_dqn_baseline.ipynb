{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "945203ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "009e2a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_run(base_path):\n",
    "    base_path = Path(base_path)\n",
    "    run_dirs = [d for d in base_path.glob(\"run_*\") if d.is_dir()]\n",
    "    if not run_dirs:\n",
    "        raise FileNotFoundError(f\"No run directories found in {base_path}\")\n",
    "    return max(run_dirs, key=lambda d: datetime.strptime(d.name.replace(\"run_\", \"\"), \"%Y-%m-%d_%H-%M-%S\"))\n",
    "\n",
    "def load_monitor_file(monitor_dir):\n",
    "    monitor_dir = Path(monitor_dir)\n",
    "    csv_file = list(monitor_dir.glob(\"*.csv\"))\n",
    "    if not csv_file:\n",
    "        raise FileNotFoundError(f\"No CSV files found in {monitor_dir}\")\n",
    "    return pd.read_csv(csv_file[0], skiprows=1)\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    rewards_df = df['r']\n",
    "    total_timesteps = df['l'].sum()\n",
    "    return {\n",
    "        'mean': rewards_df.mean(),\n",
    "        'std': rewards_df.std(),\n",
    "        'median': rewards_df.median(),\n",
    "        'max': rewards_df.max(),\n",
    "        'success': np.mean(rewards_df >= 475) * 100,\n",
    "        'timesteps':total_timesteps\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2be48f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(__file__).resolve().parents[2] if '__file__' in globals() else Path.cwd().parents[1]\n",
    "docs = root / \"documentation\" / \"cartpole\"\n",
    "\n",
    "dqn_run = find_latest_run(docs / \"dqn-cartpole\")\n",
    "base_run = find_latest_run(docs / \"random-baseline\")\n",
    "\n",
    "dqn_df = load_monitor_file(dqn_run / \"monitor\")\n",
    "base_df = load_monitor_file(base_run / \"monitor\")\n",
    "\n",
    "comparison_dir = docs / \"comparison\" / f\"dqn_vs_baseline_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "graphs_dir = comparison_dir / \"graphs\"\n",
    "graphs_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3faf705",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_metrics = calculate_metrics(dqn_df)\n",
    "base_metrics = calculate_metrics(base_df)\n",
    "improvement = (dqn_metrics['mean'] - base_metrics['mean']) / base_metrics['mean'] * 100\n",
    "t_stat, p_val = stats.ttest_ind(dqn_df['r'], base_df['r'],equal_var=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79af84d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(dqn_metrics, base_metrics, out):\n",
    "    labels = [\"Mean\", \"Median\", \"Max\", \"Success Rate (%)\"]\n",
    "    dqn_vals = [dqn_metrics['mean'], dqn_metrics['median'], dqn_metrics['max'], dqn_metrics['success']]\n",
    "    base_vals = [base_metrics['mean'], base_metrics['median'], base_metrics['max'], base_metrics['success']]\n",
    "    x = np.arange(len(labels)); w = 0.35\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.bar(x - w/2, dqn_vals, w, label='DQN', color='tab:blue')\n",
    "    plt.bar(x + w/2, base_vals, w, label='Baseline', color='tab:red')\n",
    "    plt.xticks(x, labels); plt.ylabel(\"Value\"); plt.title(\"Performance Metrics Comparison\")\n",
    "    plt.legend(); plt.grid(alpha=0.3, axis='y')\n",
    "    plt.savefig(out / \"metrics.png\", dpi=300, bbox_inches='tight'); plt.close()\n",
    "\n",
    "def plot_cumulative(dqn, base, out):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(np.cumsum(dqn['r']), label=\"DQN (Cumulative)\", color='tab:blue')\n",
    "    plt.plot(np.cumsum(base['r']), label=\"Baseline (Cumulative)\", color='tab:red')\n",
    "    plt.title(\"Cumulative Rewards Over Episodes\")\n",
    "    plt.xlabel(\"Episode\"); plt.ylabel(\"Total Reward\")\n",
    "    plt.legend(); plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out / \"cumulative.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_overlay_with_mean(dqn, base, out):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(dqn['r'].rolling(10).mean(), label=\"DQN (MA-10)\", color='tab:blue')\n",
    "    plt.plot(base['r'].rolling(10).mean(), label=\"Baseline (MA-10)\", color='tab:red')\n",
    "\n",
    "    plt.axhline(dqn['r'].mean(), color='blue', linestyle='--', alpha=0.5, label=f\"DQN Mean ({dqn['r'].mean():.1f})\")\n",
    "    plt.axhline(base['r'].mean(), color='red', linestyle='--', alpha=0.5, label=f\"Baseline Mean ({base['r'].mean():.1f})\")\n",
    "\n",
    "    plt.title(\"Moving Averages + Mean Reward Levels\")\n",
    "    plt.xlabel(\"Episode\"); plt.ylabel(\"Reward\")\n",
    "    plt.legend(); plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out / \"overlay_with_mean.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10311e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(dqn_metrics, base_metrics, graphs_dir)\n",
    "plot_cumulative(dqn_df, base_df, graphs_dir)\n",
    "plot_overlay_with_mean(dqn_df, base_df, graphs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ded08ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*DQN v Random Baseline Comparison*\n",
      "DQN Mean Reward: 69.37\n",
      "Baseline Mean Reward: 22.91\n",
      "Improvement: 202.72%\n",
      "t-stat: 16.277, p-value: 0.0000000\n",
      "DQN total timesteps: 100,100\n",
      "Baseline total timesteps: 4,583\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n*DQN v Random Baseline Comparison*\")\n",
    "print(f\"DQN Mean Reward: {dqn_metrics['mean']:.2f}\")\n",
    "print(f\"Baseline Mean Reward: {base_metrics['mean']:.2f}\")\n",
    "print(f\"Improvement: {improvement:.2f}%\")\n",
    "print(f\"t-stat: {t_stat:.3f}, p-value: {p_val:.7f}\")\n",
    "print(f\"DQN total timesteps: {dqn_metrics['timesteps']:,}\")\n",
    "print(f\"Baseline total timesteps: {base_metrics['timesteps']:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "370a2364",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_file = comparison_dir / \"summary.md\"\n",
    "with open(summary_file, \"w\") as f:\n",
    "    f.write(f\"# DQN vs Random Baseline Comparison ({datetime.now():%Y-%m-%d %H:%M:%S})\\n\\n\")\n",
    "    f.write(f\"- DQN Mean Reward: **{dqn_metrics['mean']:.2f}**\\n\")\n",
    "    f.write(f\"- Baseline Mean Reward: **{base_metrics['mean']:.2f}**\\n\")\n",
    "    f.write(f\"- Improvement: **{improvement:.2f}%**\\n\")\n",
    "    f.write(f\"- t-stat: {t_stat:.3f}\\n\")\n",
    "    f.write(f\"- p-value: {p_val:.9f}\\n\\n\")\n",
    "    f.write(f\"# Uses these runs:\\n\")\n",
    "    f.write(f\"- dqn route: {dqn_run}\\n\")\n",
    "    f.write(f\"- baseline route: {base_run}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_mac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
