{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "095ba7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4396a837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_run(base_path):\n",
    "    base_path = Path(base_path)\n",
    "    run_dirs = [d for d in base_path.glob(\"run_*\") if d.is_dir()]\n",
    "    if not run_dirs:\n",
    "        raise FileNotFoundError(f\"No run directories found in {base_path}\")\n",
    "    return max(run_dirs, key=lambda d: datetime.strptime(d.name.replace(\"run_\", \"\"), \"%Y-%m-%d_%H-%M-%S\"))\n",
    "\n",
    "def load_monitor_file(monitor_dir):\n",
    "    monitor_dir = Path(monitor_dir)\n",
    "    csv_file = list(monitor_dir.glob(\"*.csv\"))\n",
    "    if not csv_file:\n",
    "        raise FileNotFoundError(f\"No CSV files found in {monitor_dir}\")\n",
    "    return pd.read_csv(csv_file[0], skiprows=1)\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    rewards_df = df['r']\n",
    "    total_timesteps = df['l'].sum()\n",
    "    return {\n",
    "        'mean': rewards_df.mean(),\n",
    "        'std': rewards_df.std(),\n",
    "        'median': rewards_df.median(),\n",
    "        'max': rewards_df.max(),\n",
    "        \"episodes\": len(rewards_df),\n",
    "        'success': np.mean(rewards_df > 0) * 100,\n",
    "        'timesteps': total_timesteps\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "81927fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(__file__).resolve().parents[2] if '__file__' in globals() else Path.cwd().parents[1]\n",
    "docs = root / \"documentation\" / \"frozenlake\"\n",
    "\n",
    "algorithms = {\n",
    "    \"Baseline\": docs / \"random-baseline\",\n",
    "    \"A2C\": docs / \"a2c-frozenlake\",\n",
    "    \"PPO\": docs / \"ppo-frozenlake\",\n",
    "    \"DQN\": docs / \"dqn-frozenlake\",    \n",
    "}\n",
    "\n",
    "dfs = {}\n",
    "metrics = {}\n",
    "runs = {}\n",
    "\n",
    "for name, path in algorithms.items():\n",
    "    run = find_latest_run(path)\n",
    "    df = load_monitor_file(run / \"monitor\")\n",
    "    dfs[name] = df\n",
    "    runs[name] = run\n",
    "    metrics[name] = calculate_metrics(df)\n",
    "\n",
    "comparison_dir = docs / \"comparison\" / f\"algorithms_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "graphs_dir = comparison_dir / \"graphs\"\n",
    "graphs_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e81836cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics, out):\n",
    "    labels = [\"Mean\", \"Median\", \"Max\", \"Success Rate (%)\"]\n",
    "    algorithms = list(metrics.keys())\n",
    "\n",
    "    data = {\n",
    "        name: [\n",
    "            metrics[name]['mean'],\n",
    "            metrics[name].get('median', 0),\n",
    "            metrics[name]['max'],\n",
    "            metrics[name]['success'],\n",
    "        ]\n",
    "        for name in algorithms\n",
    "    }\n",
    "\n",
    "    x = np.arange(len(labels))\n",
    "    w = 0.18 \n",
    "\n",
    "    colors = {\n",
    "        \"Baseline\": \"#999999\",\n",
    "        \"A2C\": \"#00b3f4\",     \n",
    "        \"PPO\": \"#da1919\",     \n",
    "        \"DQN\": \"#17aa07\",     \n",
    "    }\n",
    "\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    for i, name in enumerate(algorithms):\n",
    "        plt.bar(\n",
    "            x + (i - len(algorithms)/2) * w + w/2,\n",
    "            data[name],\n",
    "            width=w,\n",
    "            label=name,\n",
    "            color=colors.get(name, f\"C{i}\"),\n",
    "            alpha=0.9\n",
    "        )\n",
    "\n",
    "    plt.xticks(x, labels, fontsize=10)\n",
    "    plt.ylabel(\"Value\", fontsize=11)\n",
    "    plt.title(\"FrozenLake — Algorithm Performance Metrics\", fontsize=13, weight=\"bold\")\n",
    "    plt.legend(frameon=False, fontsize=9, ncol=2)\n",
    "    plt.grid(axis='y', alpha=0.25)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out / \"metrics.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_cumulative(dfs, out):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    colors = [\"#999999\", \"#00b3f4\", \"#da1919\", \"#17aa07\"]\n",
    "    for (name, df), c in zip(dfs.items(), colors):\n",
    "        plt.plot(np.cumsum(df[\"r\"]), label=name, color=c, linewidth=1.8)\n",
    "    plt.title(\"Cumulative Rewards — FrozenLake\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Cumulative Reward\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out / \"cumulative.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_overlay_with_mean(dfs, out, window=100):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    colors = {\n",
    "        \"Baseline\": \"#999999\",\n",
    "        \"A2C\": \"#00b3f4\",     \n",
    "        \"PPO\": \"#da1919\",     \n",
    "        \"DQN\": \"#17aa07\",     \n",
    "    }\n",
    "\n",
    "    for name, df in dfs.items():\n",
    "        smooth = df[\"r\"].rolling(window).mean()\n",
    "        plt.plot(\n",
    "            smooth,\n",
    "            label=f\"{name} (MA-{window})\",\n",
    "            color=colors[name],\n",
    "            linewidth=2,\n",
    "            alpha=0.8\n",
    "        )\n",
    "\n",
    "        plt.axhline(\n",
    "            df[\"r\"].mean(),\n",
    "            color=colors[name],\n",
    "            linestyle=\"--\",\n",
    "            alpha=0.5,\n",
    "            linewidth=1\n",
    "        )\n",
    "\n",
    "    plt.title(f\"Smoothed Learning Curves (MA-{window}) — FrozenLake\", fontsize=13, pad=10)\n",
    "    plt.xlabel(\"Episode\", fontsize=11)\n",
    "    plt.ylabel(\"Average Reward\", fontsize=11)\n",
    "    plt.legend(frameon=False, fontsize=9)\n",
    "    plt.grid(alpha=0.25)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out / \"smoothed_learning.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0909669",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metrics, graphs_dir)\n",
    "plot_cumulative(dfs, graphs_dir)\n",
    "plot_overlay_with_mean(dfs, graphs_dir, window=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed6357ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_pairs = [(\"Baseline\", \"A2C\"), (\"Baseline\", \"PPO\"), (\"Baseline\", \"DQN\"),\n",
    "         (\"A2C\", \"PPO\"), (\"A2C\", \"DQN\"), (\"PPO\", \"DQN\")]\n",
    "t_results = [(a, b, *stats.ttest_ind(dfs[a][\"r\"], dfs[b][\"r\"], equal_var=False)) for a, b in compare_pairs]\n",
    "\n",
    "summary = comparison_dir / \"summary.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5856d19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FrozenLake comparison saved to /Users/davidjayakumar/Library/CloudStorage/OneDrive-AtlanticTU/YEAR 4/Project Engineering/Network-Defender/documentation/frozenlake/comparison/algorithms_2025-10-28_15-53-10\n"
     ]
    }
   ],
   "source": [
    "with open(summary, \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "    f.write(f\"# FrozenLake Comparison - ({datetime.now():%Y-%m-%d %H:%M:%S})\\n\\n\")\n",
    "    f.write(\"| Algorithm | Mean | Max | Std | Success % | Episodes |\\n\")\n",
    "    f.write(\"|------------|------|-----|-----|------------|-----------|\\n\")\n",
    "    \n",
    "    for n, m in metrics.items():\n",
    "        f.write(f\"| {n} | {m['mean']:.3f} | {m['max']:.1f} | {m['std']:.3f} | {m['success']:.2f} | {m['episodes']} |\\n\")\n",
    "\n",
    "    f.write(\"\\n## Welch’s t-test Results\\n\")\n",
    "    for a, b, t, p in t_results:\n",
    "        f.write(f\"- {a} vs {b}: t = {t:.3f}, p = {p:.6f}\\n\")\n",
    "        \n",
    "    f.write(\"\\n## Run Sources\\n\")\n",
    "    for k, v in runs.items():\n",
    "        f.write(f\"- {k}: {v}\\n\")\n",
    "\n",
    "print(f\"\\nFrozenLake comparison saved to {comparison_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_mac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
